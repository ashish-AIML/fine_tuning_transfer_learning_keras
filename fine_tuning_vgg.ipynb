{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuning_vgg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJLrtonQveV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0a4a5d76-4cdc-4889-9144-45c17f3c1dd3"
      },
      "source": [
        "!wget -c https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-17 01:58:26--  https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.144.173\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.144.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 580495262 (554M) [application/zip]\n",
            "Saving to: ‘Cat_Dog_data.zip’\n",
            "\n",
            "Cat_Dog_data.zip    100%[===================>] 553.60M  50.9MB/s    in 11s     \n",
            "\n",
            "2020-02-17 01:58:37 (49.2 MB/s) - ‘Cat_Dog_data.zip’ saved [580495262/580495262]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10QzflsIvqxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq Cat_Dog_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snNM-Z-AsNzk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "62fe5690-0776-4dd3-be8f-a38d555a5c9d"
      },
      "source": [
        "\"\"\"\n",
        "VGG16 model for Keras.\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import warnings\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras import backend as K\n",
        "from keras_applications.imagenet_utils import decode_predictions\n",
        "from keras_applications.imagenet_utils import preprocess_input\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "\n",
        "\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "\n",
        "def VGG16(include_top=True, weights='imagenet',\n",
        "          input_tensor=None, input_shape=None,\n",
        "          pooling=None,\n",
        "          classes=1000):\n",
        "  \n",
        "    if weights not in {'imagenet', None}:\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or `imagenet` '\n",
        "                         '(pre-training on ImageNet).')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=224,\n",
        "                                      min_size=48,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top,\n",
        "                                      weights=weights)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "    if include_top:\n",
        "        # Classification block\n",
        "        x = Flatten(name='flatten')(x)\n",
        "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
        "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "        elif pooling == 'max':\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "    model = Model(inputs, x, name='vgg16')\n",
        "    \n",
        "\n",
        "    # load weights\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                                    WEIGHTS_PATH,\n",
        "                                    cache_subdir='models',\n",
        "                                    file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
        "        else:\n",
        "            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                                    WEIGHTS_PATH_NO_TOP,\n",
        "                                    cache_subdir='models',\n",
        "                                    file_hash='6d6bbae143d832006294945121d1f1fc')\n",
        "        model.load_weights(weights_path)\n",
        "        if K.backend() == 'theano':\n",
        "            layer_utils.convert_all_kernels_in_model(model)\n",
        "\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            if include_top:\n",
        "                maxpool = model.get_layer(name='block5_pool')\n",
        "                shape = maxpool.output_shape[1:]\n",
        "                dense = model.get_layer(name='fc1')\n",
        "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
        "\n",
        "            if K.backend() == 'tensorflow':\n",
        "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
        "                              'are using the Theano '\n",
        "                              'image data format convention '\n",
        "                              '(`image_data_format=\"channels_first\"`). '\n",
        "                              'For best performance, set '\n",
        "                              '`image_data_format=\"channels_last\"` in '\n",
        "                              'your Keras config '\n",
        "                              'at ~/.keras/keras.json.')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkJGpuWQxh8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4aa09a4-ae6f-4dc2-c11e-488f05e09200"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras import optimizers \n",
        "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
        "import argparse\n",
        "from time import time\n",
        "\n",
        "\n",
        "#################################################################################################################################\n",
        "#################################################################################################################################\n",
        "# Following steps are required to fine-tune the model\n",
        "#\n",
        "#  1. Specify the path to training and testing data, along with number of classes and image size.\n",
        "#  2. Do some random image transformations to increase the number of training samples and load the training and testing data\n",
        "#  3. Create VGG16 network graph(without top) and load imageNet pre-trained weights\n",
        "#  4. Add the top based on number of classes we have to the network created in step-3\n",
        "#  5. Specify the optimizer, loss etc and start the training\n",
        "##################################################################################################################################\n",
        "##################################################################################################################################\n",
        "\n",
        "##### Step-1:\n",
        "############ Specify path to training and testing data. Minimum 100 images per class recommended.\n",
        "############ Default image size is 160    \n",
        "img_size=224\n",
        "\n",
        "##### Step-2:\n",
        "############ Do some random image transformations to increase the number of training samples\n",
        "############ Note that we are scaling the image to make all the values between 0 and 1. That's how our pretrained weights have been done too\n",
        "############ Default batch size is 8 but you can reduce/increase it depending on how powerful your machine is. \n",
        "\n",
        "batch_size=10\n",
        "\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "#        width_shift_range=0.1,\n",
        "#        height_shift_range=0.1,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = image.ImageDataGenerator(rescale=1. / 255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/Cat_Dog_data/train/',\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/Cat_Dog_data/test/',\n",
        "        target_size=(img_size,img_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n",
        "##### Step-3:\n",
        "############ Create VGG-16 network graph without the last layers and load imagenet pretrained weights\n",
        "############ Default image size is 160    \n",
        "print('loading the model and the pre-trained weights...')\n",
        "\n",
        "base_model = VGG16(include_top=False, weights='imagenet')\n",
        "## Here we will print the layers in the network\n",
        "i=0\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    i = i+1\n",
        "    print(i,layer.name)\n",
        "#sys.exit()\n",
        "\n",
        "##### Step-4:\n",
        "############ Add the top as per number of classes in our dataset\n",
        "############ Note that we are using Dropout layer with value of 0.2, i.e. we are discarding 20% weights\n",
        "############\n",
        "\n",
        "x = base_model.output\n",
        "x = Dense(128)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "\n",
        "##### Step-5:\n",
        "############ Specify the complete model input and output, optimizer and loss\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
        "\n",
        "filepath = 'VGG_pretrained_model.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,save_best_only=True,save_weights_only=False, mode='min',period=1)\n",
        "callbacks_list = [checkpoint,tensorboard]\n",
        "\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=0.001, momentum=0.9),metrics=[\"accuracy\"])\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(),metrics=[\"accuracy\"])\n",
        "#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0),metrics=[\"accuracy\"])\n",
        "\n",
        "num_training_img=22500\n",
        "num_validation_img=2500\n",
        "stepsPerEpoch = num_training_img/batch_size\n",
        "validationSteps= num_validation_img/batch_size\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=stepsPerEpoch,\n",
        "        epochs=5,\n",
        "        callbacks = callbacks_list,\n",
        "        validation_data = validation_generator,\n",
        "        validation_steps=validationSteps\n",
        "        )\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22500 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n",
            "loading the model and the pre-trained weights...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1 input_1\n",
            "2 block1_conv1\n",
            "3 block1_conv2\n",
            "4 block1_pool\n",
            "5 block2_conv1\n",
            "6 block2_conv2\n",
            "7 block2_pool\n",
            "8 block3_conv1\n",
            "9 block3_conv2\n",
            "10 block3_conv3\n",
            "11 block3_pool\n",
            "12 block4_conv1\n",
            "13 block4_conv2\n",
            "14 block4_conv3\n",
            "15 block4_pool\n",
            "16 block5_conv1\n",
            "17 block5_conv2\n",
            "18 block5_conv3\n",
            "19 block5_pool\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/5\n",
            "2250/2250 [==============================] - 356s 158ms/step - loss: 0.2766 - acc: 0.8806 - val_loss: 0.2073 - val_acc: 0.9108\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.20727, saving model to VGG_pretrained_model.h5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/5\n",
            "2250/2250 [==============================] - 341s 152ms/step - loss: 0.2268 - acc: 0.9029 - val_loss: 0.2384 - val_acc: 0.9004\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.20727\n",
            "Epoch 3/5\n",
            "2250/2250 [==============================] - 338s 150ms/step - loss: 0.2137 - acc: 0.9107 - val_loss: 0.2065 - val_acc: 0.9144\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.20727 to 0.20652, saving model to VGG_pretrained_model.h5\n",
            "Epoch 4/5\n",
            "2250/2250 [==============================] - 340s 151ms/step - loss: 0.2099 - acc: 0.9110 - val_loss: 0.2017 - val_acc: 0.9112\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.20652 to 0.20166, saving model to VGG_pretrained_model.h5\n",
            "Epoch 5/5\n",
            "2250/2250 [==============================] - 347s 154ms/step - loss: 0.2076 - acc: 0.9128 - val_loss: 0.1911 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.20166 to 0.19113, saving model to VGG_pretrained_model.h5\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, None, 128)   65664     \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 14,780,610\n",
            "Trainable params: 65,922\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-eJ9BXptAKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "38ec052f-5706-49b0-d55d-dbf4a1f2a8f2"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras import optimizers \n",
        "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
        "from time import time\n",
        "\n",
        "\n",
        "#################################################################################################################################\n",
        "#################################################################################################################################\n",
        "# Following steps are required to fine-tune the model\n",
        "#  1. Specify the path to training and testing data, along with number of classes and image size.\n",
        "#  2. Do some random image transformations to increase the number of training samples and load the training and testing data\n",
        "#  3. Create VGG16 network graph(without top) and don't load any weights\n",
        "#  4. Add the top based on number of classes we have. \n",
        "#  5. Load the weights saved in pre-training phase. Specify the optimizer, loss etc and start the training\n",
        "##################################################################################################################################\n",
        "##################################################################################################################################\n",
        "\n",
        "##### Step-1:\n",
        "############ Specify path to training and testing data. Minimum 100 images per class recommended.\n",
        "############ Default image size is 160    \n",
        "img_size=224\n",
        "\n",
        "##### Step-2:\n",
        "############ Do some random image transformations to increase the number of training samples\n",
        "############ Note that we are scaling the image to make all the values between 0 and 1. That's how our pretrained weights have been done too\n",
        "############ Default batch size is 8 but you can reduce/increase it depending on how powerful your machine is. \n",
        "\n",
        "batch_size=10\n",
        "\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = image.ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/Cat_Dog_data/train/',\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/Cat_Dog_data/test/',\n",
        "        target_size=(img_size,img_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n",
        "##### Step-3:\n",
        "############ Create VGG-16 network graph without the last layers and load imagenet pretrained weights\n",
        "############ Default image size is 160    \n",
        "print('loading the model and the pre-trained weights...')\n",
        "\n",
        "base_model = VGG16(include_top=False, weights=None)\n",
        "## Here we will print the layers in the network\n",
        "i=0\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "    i = i+1\n",
        "    print(i,layer.name)\n",
        "#sys.exit()\n",
        "\n",
        "##### Step-4:\n",
        "############ Add the top as per number of classes in our dataset\n",
        "############ Note that we are using Dropout layer with value of 0.2, i.e. we are discarding 20% weights\n",
        "############\n",
        "\n",
        "x = base_model.output\n",
        "x = Dense(128)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "\n",
        "##### Step-5:\n",
        "############ Specify the complete model input and output, optimizer and loss\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
        "\n",
        "filepath = '/content/VGG_pretrained_model.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,save_best_only=False,save_weights_only=False, mode='auto',period=1)\n",
        "callbacks_list = [checkpoint,tensorboard]\n",
        "\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.load_weights(\"/content/VGG_pretrained_model.h5\")\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=0.0001, momentum=0.9),metrics=[\"accuracy\"])\n",
        "\n",
        "num_training_img=22500\n",
        "num_validation_img=2500\n",
        "stepsPerEpoch = num_training_img/batch_size\n",
        "validationSteps= num_validation_img/batch_size\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=stepsPerEpoch,\n",
        "        epochs=5,\n",
        "        callbacks = callbacks_list,\n",
        "        validation_data = validation_generator,\n",
        "        validation_steps=validationSteps\n",
        "        )\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22500 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n",
            "loading the model and the pre-trained weights...\n",
            "1 input_4\n",
            "2 block1_conv1\n",
            "3 block1_conv2\n",
            "4 block1_pool\n",
            "5 block2_conv1\n",
            "6 block2_conv2\n",
            "7 block2_pool\n",
            "8 block3_conv1\n",
            "9 block3_conv2\n",
            "10 block3_conv3\n",
            "11 block3_pool\n",
            "12 block4_conv1\n",
            "13 block4_conv2\n",
            "14 block4_conv3\n",
            "15 block4_pool\n",
            "16 block5_conv1\n",
            "17 block5_conv2\n",
            "18 block5_conv3\n",
            "19 block5_pool\n",
            "Epoch 1/5\n",
            "2250/2250 [==============================] - 489s 217ms/step - loss: 0.0790 - acc: 0.9689 - val_loss: 0.0414 - val_acc: 0.9856\n",
            "\n",
            "Epoch 00001: saving model to /content/VGG_pretrained_model.h5\n",
            "Epoch 2/5\n",
            "2250/2250 [==============================] - 487s 217ms/step - loss: 0.0434 - acc: 0.9839 - val_loss: 0.0378 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00002: saving model to /content/VGG_pretrained_model.h5\n",
            "Epoch 3/5\n",
            "2250/2250 [==============================] - 489s 218ms/step - loss: 0.0346 - acc: 0.9873 - val_loss: 0.0367 - val_acc: 0.9852\n",
            "\n",
            "Epoch 00003: saving model to /content/VGG_pretrained_model.h5\n",
            "Epoch 4/5\n",
            "2250/2250 [==============================] - 490s 218ms/step - loss: 0.0264 - acc: 0.9899 - val_loss: 0.0288 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00004: saving model to /content/VGG_pretrained_model.h5\n",
            "Epoch 5/5\n",
            "2250/2250 [==============================] - 489s 218ms/step - loss: 0.0228 - acc: 0.9920 - val_loss: 0.0305 - val_acc: 0.9888\n",
            "\n",
            "Epoch 00005: saving model to /content/VGG_pretrained_model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2974a1b550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxJdjPSotBny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10f5c59f-be7a-416b-b8b2-19f7e143b88f"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import numpy as np\n",
        "\n",
        "base_model = VGG16(include_top=False, weights=None)\n",
        "x = base_model.output\n",
        "x = Dense(128)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.load_weights(\"/content/VGG_pretrained_model.h5\")\n",
        "\n",
        "inputShape = (224,224) # Assumes 3 channel image\n",
        "image = load_img('/content/Cat_Dog_data/test/cat/cat.10032.jpg', target_size=inputShape)\n",
        "image = img_to_array(image)   # shape is (224,224,3)\n",
        "image = np.expand_dims(image, axis=0)  # Now shape is (1,224,224,3)\n",
        "\n",
        "image = image/255.0\n",
        "\n",
        "preds = model.predict(image)\n",
        "print(preds)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9.999999e-01 6.641747e-08]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}